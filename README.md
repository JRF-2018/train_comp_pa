# Predictor - Actor (- Recollector) モデルと負の学習

<!-- Time-stamp: "2020-04-08T06:35:28Z" -->

今回も失敗した実験である。改造した MountainCarContinuous のアルゴリズ
ミックな学習で、アルゴリズミックな中では、確かに部分的な成功もあるので
あるが、しかし、ランダムで単純な学習と比較した場合、早さはもちろん、学
習が進むためのステップ数でも、最終的な到達点でも、アルゴリズミックな方
法のほうが負けてしまったのだった。悔しいが、今回のアルゴリズミックな方
法は、まったくの無駄だったと結論せざるを得ない。何か活かせる方法を今後、
考えたい。

そのアルゴリズミックな学習とは、まず第一に、「環境」+「行動の提案」→
「予想」を行う Predictor を学習しておき、その誤差逆伝播法を利用して
「環境」+「予想」→「行動の提案」を行う Actor を学習するというもの。逆
伝播がちゃんと使えるかが問題としてあったが、概ね使えるという実験結果を
得た。

次に、Predictor と Actor の同時学習も試みた。その際に、Actor の「行動
の提案」と、Predictor から逆伝播された「行動の提案」がどちらが良いかと
いうのが以前私がレポートした「競争的学習」のようになっていると考えた。
それについて「負の学習」を試み、それが学習を早くするという結果を得た。

しかし、この二つのアルゴリズミックな学習の結果・部分的な成功は、先に述
べたように、ランダムで単純な学習と比べると負けていたのだった。


詳しいことと更新情報は↓で。

《Predictor - Actor (- Recollector) モデルと負の学習 - JRF のソフトウェア Tips》  
http://jrf.cocolog-nifty.com/software/2020/02/post-c87651.html

続きがあれば↑から辿れるはず。00_README.txt に同等の説明もある。


## GitHub 登録までの略歴

2020-02-07、初公開。2020-02-07、バージョン 0.0.1。2020-04-08、GitHub に
バージョン 0.0.1 を初登録。


## ライセンス

パブリックドメイン。 (数式のような小さなプログラムなので。)

自由に改変・公開してください。


----
(This document is mainly written in Japanese/UTF8.)
